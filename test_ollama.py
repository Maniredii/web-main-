#!/usr/bin/env python3
"""
Test Ollama setup for job application system
"""
import os
import sys
import subprocess
from dotenv import load_dotenv

# Load environment variables
load_dotenv()
sys.path.append('src')

def test_ollama_installation():
    """Test if Ollama is installed"""
    print("ğŸ§ª Testing Ollama Installation...")
    
    try:
        result = subprocess.run(['ollama', '--version'], capture_output=True, text=True)
        if result.returncode == 0:
            print(f"âœ… Ollama installed: {result.stdout.strip()}")
            return True
        else:
            print("âŒ Ollama not responding")
            return False
    except FileNotFoundError:
        print("âŒ Ollama not found")
        print("ğŸ’¡ Download from: https://ollama.com/download/windows")
        return False

def test_ollama_models():
    """Test available Ollama models"""
    print("\nğŸ§ª Testing Ollama Models...")
    
    try:
        result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)
        if result.returncode == 0:
            models = result.stdout.strip()
            if models and 'NAME' in models:
                print("âœ… Available models:")
                print(models)
                return True
            else:
                print("âš ï¸  No models installed")
                print("ğŸ’¡ Install a model with: ollama pull llama3.1:8b")
                return False
        else:
            print("âŒ Cannot list models")
            return False
    except Exception as e:
        print(f"âŒ Error: {e}")
        return False

def test_ollama_api():
    """Test Ollama API connection"""
    print("\nğŸ§ª Testing Ollama API...")
    
    try:
        from utils import llm_provider
        
        llm = llm_provider.get_llm_model(
            provider="ollama",
            model_name="llama3.1:8b",
            temperature=0.0
        )
        
        print("âœ… LLM provider initialized")
        
        # Test a simple query
        from langchain_core.messages import HumanMessage
        
        response = llm.invoke([HumanMessage(content="Say 'Hello from Ollama!' and nothing else.")])
        print(f"âœ… API Response: {response.content}")
        
        return True
        
    except Exception as e:
        print(f"âŒ API Error: {e}")
        return False

def install_recommended_model():
    """Install recommended model for job applications"""
    print("\nğŸš€ Installing recommended model...")
    print("This will download llama3.1:8b (about 4GB)")
    
    confirm = input("Continue? (y/n): ").strip().lower()
    if confirm != 'y':
        print("Skipped model installation")
        return
    
    try:
        print("ğŸ“¥ Downloading llama3.1:8b...")
        result = subprocess.run(['ollama', 'pull', 'llama3.1:8b'], 
                              capture_output=False, text=True)
        if result.returncode == 0:
            print("âœ… Model installed successfully!")
            return True
        else:
            print("âŒ Model installation failed")
            return False
    except Exception as e:
        print(f"âŒ Error: {e}")
        return False

def main():
    """Main test function"""
    print("ğŸ¤– OLLAMA SETUP TEST")
    print("=" * 30)
    
    # Test installation
    if not test_ollama_installation():
        print("\nâŒ Ollama not installed")
        print("ğŸ“– Installation Guide:")
        print("1. Go to: https://ollama.com/download/windows")
        print("2. Download and install Ollama")
        print("3. Restart your terminal")
        print("4. Run this test again")
        return
    
    # Test models
    has_models = test_ollama_models()
    
    if not has_models:
        print("\nğŸ’¡ Would you like to install the recommended model?")
        install_recommended_model()
        has_models = test_ollama_models()
    
    if has_models:
        # Test API
        api_works = test_ollama_api()
        
        if api_works:
            print("\nğŸ‰ OLLAMA IS READY!")
            print("=" * 30)
            print("âœ… Installation: Working")
            print("âœ… Models: Available")
            print("âœ… API: Working")
            print("\nğŸš€ You can now run:")
            print("python3.11 ollama_job_applier.py")
        else:
            print("\nâš ï¸  Ollama installed but API not working")
            print("ğŸ’¡ Try restarting Ollama service")
    else:
        print("\nâŒ No models available")
        print("ğŸ’¡ Install a model first")

if __name__ == "__main__":
    main()
